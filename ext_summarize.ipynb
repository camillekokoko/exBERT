{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from exBERT import BertTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from exBERT import BertModel, BertConfig\n",
    "from SUMM.encoder import ExtTransformerEncoder\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, config_2 = 'config/bert_config_ex_s3.json'):\n",
    "        super(Bert, self).__init__()\n",
    "        # self.bert_type = bert_type\n",
    "        # if bert_type == 'bertbase':\n",
    "        bert_config_1 = BertConfig.from_json_file('config/bert_config.json')\n",
    "        bert_config_2 = BertConfig.from_json_file(config_2)\n",
    "\n",
    "        self.model = BertModel(bert_config_1, bert_config_2)\n",
    "      \n",
    "    def forward(self, x, segs, mask):\n",
    "        top_vec, second_augment = self.model(x, attention_mask=mask, token_type_ids=segs)   ## top_vec = //// self.bert(src, segs, mask_src)\n",
    "        print('@@@@@', top_vec)\n",
    "        print('@@@@@second_augment', second_augment)\n",
    "        print('@@@@@@@@ top_vec size', len(top_vec))\n",
    "        print('@@@@@second_augment size', second_augment)\n",
    "\n",
    "        if len(top_vec) == 0: #debug\n",
    "            print(\"top_vec is empty\") #debug\n",
    "            print(top_vec) #debug\n",
    "        else: #debug\n",
    "            print(\"top_vec is not empty\") #debug\n",
    "        return top_vec\n",
    "\n",
    "class ExtSummarizer(nn.Module):\n",
    "    def __init__(self, device = 'cpu', checkpoint = None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.bert = Bert()\n",
    "        self.load_state_dict(checkpoint, strict = False)\n",
    "\n",
    "        \n",
    "        self.ext_layer = ExtTransformerEncoder(\n",
    "            self.bert.model.config.hidden_size, d_ff=2048, heads=8, dropout=0.2, num_inter_layers=2\n",
    "        )\n",
    "        \n",
    "        # print(self) #debug\n",
    "        \n",
    "        # print('ext_layer', self.ext_layer) #debug\n",
    "        # print('checkpoint', checkpoint)  #debug\n",
    "        #self.load_state_dict(checkpoint, strict = False)\n",
    "        # new_model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "\n",
    "        # Match and load BERT layers\n",
    "        for model_layer_name, model_layer in self.bert.named_modules():\n",
    "            if model_layer_name in checkpoint:\n",
    "                model_layer.load_state_dict(checkpoint[model_layer_name])\n",
    "        \n",
    "        # Match and load ExtTransformerEncoder layers\n",
    "        for model_layer_name, model_layer in self.ext_layer.named_modules():\n",
    "            if model_layer_name in checkpoint:\n",
    "                model_layer.load_state_dict(checkpoint[model_layer_name])\n",
    "        \n",
    "        # Move the entire model (including the BERT model) to the specified device\n",
    "        self.to(device)\n",
    "        # print('print out ExtSummarizer', self) #debug\n",
    "\n",
    "\n",
    "    def forward(self, src, segs, clss, mask_src, mask_cls):\n",
    "        # top_vec = self.bert(src, segs, mask_src) ###<----empty WHYYYYY!!!!! KILL ME PLEASE\n",
    "        print('----->check:src', src)\n",
    "        print('----->check:segs', segs)\n",
    "        print('----->check:mask_src', mask_src )\n",
    "        \n",
    "        top_vec = self.bert(src, segs, mask_src) \n",
    "        # print('EXTTTTT top_vec', top_vec)\n",
    "        print('EXTTTTT len top_vec', len(top_vec))\n",
    "        print('EXTTTTT[0]len top_vec ', len(top_vec[0]))#.size())\n",
    "        \n",
    "        print('EXTTTTT[1] len top_vec ', len(top_vec[1]))#.size())\n",
    "        print('type of top_vec', type(top_vec))\n",
    "        print('type of[0] top_vec', type(top_vec[0]))\n",
    "        print('type of[1] top_vec', type(top_vec[1]))\n",
    "        \n",
    "        print('len[0] [0] top_vec size ', len(top_vec[0][0]))\n",
    "        print('len[1] [0] top_vec size ', len(top_vec[1][0]))\n",
    "\n",
    "\n",
    "\n",
    "        # # Check if top_vec is a tuple and extract the tensor from it\n",
    "        # if isinstance(top_vec, tuple):\n",
    "        # # Assuming the tensor is the first element of the tuple\n",
    "        #     top_vec = top_vec[0]\n",
    "\n",
    "        sents_vec = top_vec[torch.arange(top_vec[1].size(0)).unsqueeze(1), clss]\n",
    "        print('sents_vec', sents_vec)\n",
    "        print('sents_vec shape', sents_vec.shape)\n",
    "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "        sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)\n",
    "        return sent_scores, mask_cls\n",
    "    \n",
    "# class AbsSummarizer(nn.Module):\n",
    "#     def __init__(self, device = 'cpu', checkpoint_pth='./results/bert_ex_s3/Best_stat_dic_exBERTe2_b32_lr1e-05.pth', bert_from_extractive=None, bert_type='bertbase') :\n",
    "#         super().__init__()\n",
    "#         self.device = device\n",
    "#         self.bert = Bert(bert_type=bert_type)\n",
    "        \n",
    "#         # if bert_from_extractive is not None:\n",
    "#         #     self.bert.model.load_state_dict(\n",
    "#         #         dict([(n[11:], p) for n, p in bert_from_extractive.items() if n.startswith('bert.model')]), strict=True)\n",
    "        \n",
    "#         self.vocab_size = self.bert.model.config.vocab_size\n",
    "#         tgt_embeddings = nn.Embedding(self.vocab_size, self.bert.model.config.hidden_size, padding_idx=0)\n",
    "        \n",
    "#         self.decoder = TransformerDecoder(\n",
    "#             self.args.dec_layers,\n",
    "#             self.args.dec_hidden_size, heads=self.args.dec_heads,\n",
    "#             d_ff=self.args.dec_ff_size, dropout=self.args.dec_dropout, embeddings=tgt_embeddings)\n",
    "        \n",
    "#         self.generator = get_generator(self.vocab_size, self.args.dec_hidden_size, device)\n",
    "#         self.generator[0].weight = self.decoder.embeddings.weight\n",
    "        \n",
    "#         self.load_state_dict(checkpoint_pth['model'], strict=True)\n",
    "        \n",
    "#         self.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(source_fp):\n",
    "    \"\"\"\n",
    "    - Remove \\n\n",
    "    - Sentence Tokenize\n",
    "    - Add [SEP] [CLS] as sentence boundary\n",
    "    \"\"\"\n",
    "    with open(source_fp) as source:\n",
    "        raw_text = source.read().replace(\"\\n\", \" \").replace(\"[CLS] [SEP]\", \" \")\n",
    "    sents = sent_tokenize(raw_text)\n",
    "    # print('nltk--sent_tokenize', sents) # debug\n",
    "    processed_text = \"[CLS] [SEP]\".join(sents)\n",
    "    print('processed_text', processed_text)\n",
    "    return processed_text, len(sents)\n",
    "\n",
    "\n",
    "def load_text(processed_text, max_pos, device):\n",
    "    # tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "    tok = BertTokenizer(\"data/interim/custom_bert_vocab.txt\")\n",
    "\n",
    "    # tokenizer.post_processor = TemplateProcessing(\n",
    "    #     single=\"[CLS] $A [SEP]\",\n",
    "    #     pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    #     special_tokens=[\n",
    "    #         (\"[CLS]\", tokenizer.convert_tokens_to_ids(\"[CLS]\")),\n",
    "    #         (\"[SEP]\", tokenizer.convert_tokens_to_ids(\"[SEP]\")),\n",
    "    #         ],\n",
    "    #     )   \n",
    "    # tokenizer.post_processor = post_processor\n",
    "\n",
    "    sep_vid = tok.vocab[\"[SEP]\"]\n",
    "    # print('sep_vid', sep_vid) # debug\n",
    "    cls_vid = tok.vocab[\"[CLS]\"]\n",
    "    # print('cls_vid', cls_vid) #debug\n",
    "\n",
    "    def _process_src(raw):\n",
    "        # raw = raw.strip().lower()\n",
    "        # raw = raw.replace(\"[cls]\", \"[CLS]\").replace(\"[sep]\", \"[SEP]\")\n",
    "        print('raw', raw)\n",
    "        \n",
    "        src_subtokens = tok.tokenize(raw)\n",
    "        print('src_subtokens_1', src_subtokens)\n",
    "        \n",
    "        # tokens = ['[', 'cl', '##s', ']', '[', 'sep', ']']\n",
    "        new_tokens = ['[CLS]' if token in ['[', 'cl', '##s', ']'] else '[SEP]' if token in ['[', 'sep', ']'] else token for token in src_subtokens]\n",
    "        new_tokens = [new_tokens[0]] + [token for i, token in enumerate(new_tokens[1:], 1) if not (token == '[CLS]' and new_tokens[i - 1] == '[CLS]')]\n",
    "\n",
    "        print('new_tokens', new_tokens)\n",
    "        \n",
    "        src_subtokens = [\"[CLS]\"] + new_tokens + [\"[SEP]\"]\n",
    "        src_subtoken_idxs = tok.convert_tokens_to_ids(src_subtokens)\n",
    "        # Original list of tokens\n",
    "\n",
    "        print('src_subtoken_idxs', src_subtoken_idxs)\n",
    "        src_subtoken_idxs = src_subtoken_idxs[:-1][:max_pos]\n",
    "        src_subtoken_idxs[-1] = sep_vid\n",
    "        print('src_subtokens', src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        \n",
    "        segments_ids = []\n",
    "        segs = segs[:max_pos]\n",
    "        for i, s in enumerate(segs):\n",
    "            if i % 2 == 0:\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        print('segments_ids', segments_ids)\n",
    "        src = torch.tensor(src_subtoken_idxs)[None, :].to(device)\n",
    "        mask_src = ((~(src == 0)).float()).to(device)\n",
    "        cls_ids = [[i for i, t in enumerate(src_subtoken_idxs) if t == cls_vid]]\n",
    "        clss = torch.tensor(cls_ids).to(device)\n",
    "        mask_cls = (~(clss == -1)).float()\n",
    "        clss[clss == -1] = 0\n",
    "        return src, mask_src, segments_ids, clss, mask_cls\n",
    "\n",
    "    src, mask_src, segments_ids, clss, mask_cls = _process_src(processed_text)\n",
    "    segs = torch.tensor(segments_ids)[None, :].to(device)\n",
    "    src_text = [[sent.replace(\"[SEP]\", \"\").strip() for sent in processed_text.split(\"[CLS]\")]]\n",
    "    return src, mask_src, segs, clss, mask_cls, src_text\n",
    "\n",
    "def test(model, input_data, result_path, max_length, block_trigram=True):\n",
    "    def _get_ngrams(n, text):\n",
    "        ngram_set = set()\n",
    "        text_length = len(text)\n",
    "        max_index_ngram_start = text_length - n\n",
    "        for i in range(max_index_ngram_start + 1):\n",
    "            ngram_set.add(tuple(text[i : i + n]))\n",
    "        return ngram_set\n",
    "\n",
    "    def _block_tri(c, p):\n",
    "        tri_c = _get_ngrams(3, c.split())\n",
    "        for s in p:\n",
    "            tri_s = _get_ngrams(3, s.split())\n",
    "            if len(tri_c.intersection(tri_s)) > 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    with open(result_path, \"w\") as save_pred:\n",
    "        with torch.no_grad():\n",
    "            src, mask, segs, clss, mask_cls, src_str = input_data\n",
    "            print('test-----src ',input_data[0])\n",
    "            print('test-----index 1',input_data[1])\n",
    "            print('test-----seg ',input_data[2])\n",
    "            print('test-----index 3',input_data[3])\n",
    "            print('test-----index 4',input_data[4])\n",
    "            print('test-----index 5',input_data[5])\n",
    "            \n",
    "            \n",
    "   \n",
    "            sent_scores, mask = model(src, segs, clss, mask, mask_cls)\n",
    "            print('test-----sent_scores',sent_scores)\n",
    "            print('test-----mask',mask)\n",
    "            print('test-----', model)\n",
    "            \n",
    "            \n",
    "            \n",
    "            sent_scores = sent_scores + mask.float()\n",
    "            sent_scores = sent_scores.cpu().data.numpy()\n",
    "            selected_ids = np.argsort(-sent_scores, 1)\n",
    "\n",
    "            pred = []\n",
    "            for i, idx in enumerate(selected_ids):\n",
    "                _pred = []\n",
    "                if len(src_str[i]) == 0:\n",
    "                    continue\n",
    "                for j in selected_ids[i][: len(src_str[i])]:\n",
    "                    if j >= len(src_str[i]):\n",
    "                        continue\n",
    "                    candidate = src_str[i][j].strip()\n",
    "                    if block_trigram:\n",
    "                        if not _block_tri(candidate, _pred):\n",
    "                            _pred.append(candidate)\n",
    "                    else:\n",
    "                        _pred.append(candidate)\n",
    "\n",
    "                    if len(_pred) == max_length:\n",
    "                        break\n",
    "\n",
    "                _pred = \" \".join(_pred)\n",
    "                pred.append(_pred)\n",
    "\n",
    "            for i in range(len(pred)):\n",
    "                save_pred.write(pred[i].strip() + \"\\n\")\n",
    "\n",
    "\n",
    "def summarize(raw_txt_fp, result_fp, model, max_length=2, max_pos=512, return_summary=True):\n",
    "    model.eval()\n",
    "    processed_text, full_length = preprocess(raw_txt_fp)\n",
    "    input_data = load_text(processed_text, max_pos, device=\"cpu\")\n",
    "    print('src ',input_data[0])\n",
    "    print('index 1',input_data[1])\n",
    "    print('seg ',input_data[2])\n",
    "    print('index 3',input_data[3])\n",
    "    print('index 4',input_data[4])\n",
    "    print('index 5',input_data[5])\n",
    "\n",
    " \n",
    "           # src, mask, segs, clss, mask_cls, src_str = input_data\n",
    "\n",
    "\n",
    "    test(model, input_data, result_fp, max_length, block_trigram=True)\n",
    "    if return_summary:\n",
    "        return open(result_fp).read().strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_text (CNN) Over and over again in 2018, during an apology tour that took him from the halls of the US Congress to an appearance before the European Parliament, Mark Zuckerberg said Facebook had failed to \"take a broad enough view of our responsibilities.[CLS] [SEP]But two years later, Zuckerberg and Facebook are still struggling with their responsibilities and how to handle one of their most famous users: President Donald Trump.[CLS] [SEP]Despite Zuckerberg having previously indicated any post that \"incites violence\" would be a line in the sand — even if it came from a politician — Facebook remained silent for hours Friday after Trump was accused of glorifying violence in posts that appeared on its platforms.[CLS] [SEP]At 12:53am ET on Friday morning, as cable news networks carried images of fires and destructive protests in Minneapolis, the President tweeted : \"These THUGS are dishonoring the memory of George Floyd, and I won't let that happen.[CLS] [SEP]Just spoke to Governor Tim Walz and told him that the Military is with him all the way.[CLS] [SEP]Any difficulty and we will assume control but, when the looting starts, the shooting starts.[CLS] [SEP]Thank you![CLS] [SEP]His phrase \"when the looting starts, the shooting starts,\" mirrors language used by a Miami police chief in the late 1960s in the wake of riots.[CLS] [SEP]Its use was immediately condemned by a wide array of individuals, from historians to members of rival political campaigns.[CLS] [SEP]Former Vice President and presumptive Democratic nominee Joe Biden said Trump was \"calling for violence against American citizens during a moment of pain for so many.[CLS] [SEP]Read More\n",
      "raw (CNN) Over and over again in 2018, during an apology tour that took him from the halls of the US Congress to an appearance before the European Parliament, Mark Zuckerberg said Facebook had failed to \"take a broad enough view of our responsibilities.[CLS] [SEP]But two years later, Zuckerberg and Facebook are still struggling with their responsibilities and how to handle one of their most famous users: President Donald Trump.[CLS] [SEP]Despite Zuckerberg having previously indicated any post that \"incites violence\" would be a line in the sand — even if it came from a politician — Facebook remained silent for hours Friday after Trump was accused of glorifying violence in posts that appeared on its platforms.[CLS] [SEP]At 12:53am ET on Friday morning, as cable news networks carried images of fires and destructive protests in Minneapolis, the President tweeted : \"These THUGS are dishonoring the memory of George Floyd, and I won't let that happen.[CLS] [SEP]Just spoke to Governor Tim Walz and told him that the Military is with him all the way.[CLS] [SEP]Any difficulty and we will assume control but, when the looting starts, the shooting starts.[CLS] [SEP]Thank you![CLS] [SEP]His phrase \"when the looting starts, the shooting starts,\" mirrors language used by a Miami police chief in the late 1960s in the wake of riots.[CLS] [SEP]Its use was immediately condemned by a wide array of individuals, from historians to members of rival political campaigns.[CLS] [SEP]Former Vice President and presumptive Democratic nominee Joe Biden said Trump was \"calling for violence against American citizens during a moment of pain for so many.[CLS] [SEP]Read More\n",
      "src_subtokens_1 ['(', 'cnn', ')', 'over', 'and', 'over', 'again', 'in', '2018', ',', 'during', 'an', 'apology', 'tour', 'that', 'took', 'him', 'from', 'the', 'halls', 'of', 'the', 'us', 'congress', 'to', 'an', 'appearance', 'before', 'the', 'european', 'parliament', ',', 'mark', 'zu', '##cker', '##berg', 'said', 'facebook', 'had', 'failed', 'to', '\"', 'take', 'a', 'broad', 'enough', 'view', 'of', 'our', 'responsibilities', '.', '[', 'cl', '##s', ']', '[', 'sep', ']', 'but', 'two', 'years', 'later', ',', 'zu', '##cker', '##berg', 'and', 'facebook', 'are', 'still', 'struggling', 'with', 'their', 'responsibilities', 'and', 'how', 'to', 'handle', 'one', 'of', 'their', 'most', 'famous', 'users', ':', 'president', 'donald', 'trump', '.', '[', 'cl', '##s', ']', '[', 'sep', ']', 'despite', 'zu', '##cker', '##berg', 'having', 'previously', 'indicated', 'any', 'post', 'that', '\"', 'inc', '##ites', 'violence', '\"', 'would', 'be', 'a', 'line', 'in', 'the', 'sand', '—', 'even', 'if', 'it', 'came', 'from', 'a', 'politician', '—', 'facebook', 'remained', 'silent', 'for', 'hours', 'friday', 'after', 'trump', 'was', 'accused', 'of', 'g', '##lor', '##ifying', 'violence', 'in', 'posts', 'that', 'appeared', 'on', 'its', 'platforms', '.', '[', 'cl', '##s', ']', '[', 'sep', ']', 'at', '12', ':', '53', '##am', 'et', 'on', 'friday', 'morning', ',', 'as', 'cable', 'news', 'networks', 'carried', 'images', 'of', 'fires', 'and', 'destructive', 'protests', 'in', 'minneapolis', ',', 'the', 'president', 't', '##wee', '##ted', ':', '\"', 'these', 'thugs', 'are', 'dish', '##ono', '##ring', 'the', 'memory', 'of', 'george', 'floyd', ',', 'and', 'i', 'won', \"'\", 't', 'let', 'that', 'happen', '.', '[', 'cl', '##s', ']', '[', 'sep', ']', 'just', 'spoke', 'to', 'governor', 'tim', 'wal', '##z', 'and', 'told', 'him', 'that', 'the', 'military', 'is', 'with', 'him', 'all', 'the', 'way', '.', '[', 'cl', '##s', ']', '[', 'sep', ']', 'any', 'difficulty', 'and', 'we', 'will', 'assume', 'control', 'but', ',', 'when', 'the', 'looting', 'starts', ',', 'the', 'shooting', 'starts', '.', '[', 'cl', '##s', ']', '[', 'sep', ']', 'thank', 'you', '!', '[', 'cl', '##s', ']', '[', 'sep', ']', 'his', 'phrase', '\"', 'when', 'the', 'looting', 'starts', ',', 'the', 'shooting', 'starts', ',', '\"', 'mirrors', 'language', 'used', 'by', 'a', 'miami', 'police', 'chief', 'in', 'the', 'late', '1960s', 'in', 'the', 'wake', 'of', 'riots', '.', '[', 'cl', '##s', ']', '[', 'sep', ']', 'its', 'use', 'was', 'immediately', 'condemned', 'by', 'a', 'wide', 'array', 'of', 'individuals', ',', 'from', 'historians', 'to', 'members', 'of', 'rival', 'political', 'campaigns', '.', '[', 'cl', '##s', ']', '[', 'sep', ']', 'former', 'vice', 'president', 'and', 'pre', '##sum', '##ptive', 'democratic', 'nominee', 'joe', 'bid', '##en', 'said', 'trump', 'was', '\"', 'calling', 'for', 'violence', 'against', 'american', 'citizens', 'during', 'a', 'moment', 'of', 'pain', 'for', 'so', 'many', '.', '[', 'cl', '##s', ']', '[', 'sep', ']', 'read', 'more']\n",
      "new_tokens ['(', 'cnn', ')', 'over', 'and', 'over', 'again', 'in', '2018', ',', 'during', 'an', 'apology', 'tour', 'that', 'took', 'him', 'from', 'the', 'halls', 'of', 'the', 'us', 'congress', 'to', 'an', 'appearance', 'before', 'the', 'european', 'parliament', ',', 'mark', 'zu', '##cker', '##berg', 'said', 'facebook', 'had', 'failed', 'to', '\"', 'take', 'a', 'broad', 'enough', 'view', 'of', 'our', 'responsibilities', '.', '[CLS]', '[SEP]', '[CLS]', 'but', 'two', 'years', 'later', ',', 'zu', '##cker', '##berg', 'and', 'facebook', 'are', 'still', 'struggling', 'with', 'their', 'responsibilities', 'and', 'how', 'to', 'handle', 'one', 'of', 'their', 'most', 'famous', 'users', ':', 'president', 'donald', 'trump', '.', '[CLS]', '[SEP]', '[CLS]', 'despite', 'zu', '##cker', '##berg', 'having', 'previously', 'indicated', 'any', 'post', 'that', '\"', 'inc', '##ites', 'violence', '\"', 'would', 'be', 'a', 'line', 'in', 'the', 'sand', '—', 'even', 'if', 'it', 'came', 'from', 'a', 'politician', '—', 'facebook', 'remained', 'silent', 'for', 'hours', 'friday', 'after', 'trump', 'was', 'accused', 'of', 'g', '##lor', '##ifying', 'violence', 'in', 'posts', 'that', 'appeared', 'on', 'its', 'platforms', '.', '[CLS]', '[SEP]', '[CLS]', 'at', '12', ':', '53', '##am', 'et', 'on', 'friday', 'morning', ',', 'as', 'cable', 'news', 'networks', 'carried', 'images', 'of', 'fires', 'and', 'destructive', 'protests', 'in', 'minneapolis', ',', 'the', 'president', 't', '##wee', '##ted', ':', '\"', 'these', 'thugs', 'are', 'dish', '##ono', '##ring', 'the', 'memory', 'of', 'george', 'floyd', ',', 'and', 'i', 'won', \"'\", 't', 'let', 'that', 'happen', '.', '[CLS]', '[SEP]', '[CLS]', 'just', 'spoke', 'to', 'governor', 'tim', 'wal', '##z', 'and', 'told', 'him', 'that', 'the', 'military', 'is', 'with', 'him', 'all', 'the', 'way', '.', '[CLS]', '[SEP]', '[CLS]', 'any', 'difficulty', 'and', 'we', 'will', 'assume', 'control', 'but', ',', 'when', 'the', 'looting', 'starts', ',', 'the', 'shooting', 'starts', '.', '[CLS]', '[SEP]', '[CLS]', 'thank', 'you', '!', '[CLS]', '[SEP]', '[CLS]', 'his', 'phrase', '\"', 'when', 'the', 'looting', 'starts', ',', 'the', 'shooting', 'starts', ',', '\"', 'mirrors', 'language', 'used', 'by', 'a', 'miami', 'police', 'chief', 'in', 'the', 'late', '1960s', 'in', 'the', 'wake', 'of', 'riots', '.', '[CLS]', '[SEP]', '[CLS]', 'its', 'use', 'was', 'immediately', 'condemned', 'by', 'a', 'wide', 'array', 'of', 'individuals', ',', 'from', 'historians', 'to', 'members', 'of', 'rival', 'political', 'campaigns', '.', '[CLS]', '[SEP]', '[CLS]', 'former', 'vice', 'president', 'and', 'pre', '##sum', '##ptive', 'democratic', 'nominee', 'joe', 'bid', '##en', 'said', 'trump', 'was', '\"', 'calling', 'for', 'violence', 'against', 'american', 'citizens', 'during', 'a', 'moment', 'of', 'pain', 'for', 'so', 'many', '.', '[CLS]', '[SEP]', '[CLS]', 'read', 'more']\n",
      "src_subtoken_idxs [101, 1006, 13229, 1007, 2058, 1998, 2058, 2153, 1999, 2760, 1010, 2076, 2019, 12480, 2778, 2008, 2165, 2032, 2013, 1996, 9873, 1997, 1996, 2149, 3519, 2000, 2019, 3311, 2077, 1996, 2647, 3323, 1010, 2928, 16950, 9102, 4059, 2056, 9130, 2018, 3478, 2000, 1000, 2202, 1037, 5041, 2438, 3193, 1997, 2256, 10198, 1012, 101, 102, 101, 2021, 2048, 2086, 2101, 1010, 16950, 9102, 4059, 1998, 9130, 2024, 2145, 8084, 2007, 2037, 10198, 1998, 2129, 2000, 5047, 2028, 1997, 2037, 2087, 3297, 5198, 1024, 2343, 6221, 8398, 1012, 101, 102, 101, 2750, 16950, 9102, 4059, 2383, 3130, 5393, 2151, 2695, 2008, 1000, 4297, 7616, 4808, 1000, 2052, 2022, 1037, 2240, 1999, 1996, 5472, 1517, 2130, 2065, 2009, 2234, 2013, 1037, 3761, 1517, 9130, 2815, 4333, 2005, 2847, 5958, 2044, 8398, 2001, 5496, 1997, 1043, 10626, 11787, 4808, 1999, 8466, 2008, 2596, 2006, 2049, 7248, 1012, 101, 102, 101, 2012, 2260, 1024, 5187, 3286, 3802, 2006, 5958, 2851, 1010, 2004, 5830, 2739, 6125, 3344, 4871, 1997, 8769, 1998, 15615, 8090, 1999, 11334, 1010, 1996, 2343, 1056, 28394, 3064, 1024, 1000, 2122, 24106, 2024, 9841, 17175, 4892, 1996, 3638, 1997, 2577, 12305, 1010, 1998, 1045, 2180, 1005, 1056, 2292, 2008, 4148, 1012, 101, 102, 101, 2074, 3764, 2000, 3099, 5199, 24547, 2480, 1998, 2409, 2032, 2008, 1996, 2510, 2003, 2007, 2032, 2035, 1996, 2126, 1012, 101, 102, 101, 2151, 7669, 1998, 2057, 2097, 7868, 2491, 2021, 1010, 2043, 1996, 29367, 4627, 1010, 1996, 5008, 4627, 1012, 101, 102, 101, 4067, 2017, 999, 101, 102, 101, 2010, 7655, 1000, 2043, 1996, 29367, 4627, 1010, 1996, 5008, 4627, 1010, 1000, 13536, 2653, 2109, 2011, 1037, 5631, 2610, 2708, 1999, 1996, 2397, 4120, 1999, 1996, 5256, 1997, 12925, 1012, 101, 102, 101, 2049, 2224, 2001, 3202, 10033, 2011, 1037, 2898, 9140, 1997, 3633, 1010, 2013, 7862, 2000, 2372, 1997, 6538, 2576, 8008, 1012, 101, 102, 101, 2280, 3580, 2343, 1998, 3653, 17421, 24971, 3537, 9773, 3533, 7226, 2368, 2056, 8398, 2001, 1000, 4214, 2005, 4808, 2114, 2137, 4480, 2076, 1037, 2617, 1997, 3255, 2005, 2061, 2116, 1012, 101, 102, 101, 3191, 2062, 102]\n",
      "src_subtokens ['[CLS]', '(', 'cnn', ')', 'over', 'and', 'over', 'again', 'in', '2018', ',', 'during', 'an', 'apology', 'tour', 'that', 'took', 'him', 'from', 'the', 'halls', 'of', 'the', 'us', 'congress', 'to', 'an', 'appearance', 'before', 'the', 'european', 'parliament', ',', 'mark', 'zu', '##cker', '##berg', 'said', 'facebook', 'had', 'failed', 'to', '\"', 'take', 'a', 'broad', 'enough', 'view', 'of', 'our', 'responsibilities', '.', '[CLS]', '[SEP]', '[CLS]', 'but', 'two', 'years', 'later', ',', 'zu', '##cker', '##berg', 'and', 'facebook', 'are', 'still', 'struggling', 'with', 'their', 'responsibilities', 'and', 'how', 'to', 'handle', 'one', 'of', 'their', 'most', 'famous', 'users', ':', 'president', 'donald', 'trump', '.', '[CLS]', '[SEP]', '[CLS]', 'despite', 'zu', '##cker', '##berg', 'having', 'previously', 'indicated', 'any', 'post', 'that', '\"', 'inc', '##ites', 'violence', '\"', 'would', 'be', 'a', 'line', 'in', 'the', 'sand', '—', 'even', 'if', 'it', 'came', 'from', 'a', 'politician', '—', 'facebook', 'remained', 'silent', 'for', 'hours', 'friday', 'after', 'trump', 'was', 'accused', 'of', 'g', '##lor', '##ifying', 'violence', 'in', 'posts', 'that', 'appeared', 'on', 'its', 'platforms', '.', '[CLS]', '[SEP]', '[CLS]', 'at', '12', ':', '53', '##am', 'et', 'on', 'friday', 'morning', ',', 'as', 'cable', 'news', 'networks', 'carried', 'images', 'of', 'fires', 'and', 'destructive', 'protests', 'in', 'minneapolis', ',', 'the', 'president', 't', '##wee', '##ted', ':', '\"', 'these', 'thugs', 'are', 'dish', '##ono', '##ring', 'the', 'memory', 'of', 'george', 'floyd', ',', 'and', 'i', 'won', \"'\", 't', 'let', 'that', 'happen', '.', '[CLS]', '[SEP]', '[CLS]', 'just', 'spoke', 'to', 'governor', 'tim', 'wal', '##z', 'and', 'told', 'him', 'that', 'the', 'military', 'is', 'with', 'him', 'all', 'the', 'way', '.', '[CLS]', '[SEP]', '[CLS]', 'any', 'difficulty', 'and', 'we', 'will', 'assume', 'control', 'but', ',', 'when', 'the', 'looting', 'starts', ',', 'the', 'shooting', 'starts', '.', '[CLS]', '[SEP]', '[CLS]', 'thank', 'you', '!', '[CLS]', '[SEP]', '[CLS]', 'his', 'phrase', '\"', 'when', 'the', 'looting', 'starts', ',', 'the', 'shooting', 'starts', ',', '\"', 'mirrors', 'language', 'used', 'by', 'a', 'miami', 'police', 'chief', 'in', 'the', 'late', '1960s', 'in', 'the', 'wake', 'of', 'riots', '.', '[CLS]', '[SEP]', '[CLS]', 'its', 'use', 'was', 'immediately', 'condemned', 'by', 'a', 'wide', 'array', 'of', 'individuals', ',', 'from', 'historians', 'to', 'members', 'of', 'rival', 'political', 'campaigns', '.', '[CLS]', '[SEP]', '[CLS]', 'former', 'vice', 'president', 'and', 'pre', '##sum', '##ptive', 'democratic', 'nominee', 'joe', 'bid', '##en', 'said', 'trump', 'was', '\"', 'calling', 'for', 'violence', 'against', 'american', 'citizens', 'during', 'a', 'moment', 'of', 'pain', 'for', 'so', 'many', '.', '[CLS]', '[SEP]', '[CLS]', 'read', 'more', '[SEP]']\n",
      "segments_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "src  tensor([[  101,  1006, 13229,  1007,  2058,  1998,  2058,  2153,  1999,  2760,\n",
      "          1010,  2076,  2019, 12480,  2778,  2008,  2165,  2032,  2013,  1996,\n",
      "          9873,  1997,  1996,  2149,  3519,  2000,  2019,  3311,  2077,  1996,\n",
      "          2647,  3323,  1010,  2928, 16950,  9102,  4059,  2056,  9130,  2018,\n",
      "          3478,  2000,  1000,  2202,  1037,  5041,  2438,  3193,  1997,  2256,\n",
      "         10198,  1012,   101,   102,   101,  2021,  2048,  2086,  2101,  1010,\n",
      "         16950,  9102,  4059,  1998,  9130,  2024,  2145,  8084,  2007,  2037,\n",
      "         10198,  1998,  2129,  2000,  5047,  2028,  1997,  2037,  2087,  3297,\n",
      "          5198,  1024,  2343,  6221,  8398,  1012,   101,   102,   101,  2750,\n",
      "         16950,  9102,  4059,  2383,  3130,  5393,  2151,  2695,  2008,  1000,\n",
      "          4297,  7616,  4808,  1000,  2052,  2022,  1037,  2240,  1999,  1996,\n",
      "          5472,  1517,  2130,  2065,  2009,  2234,  2013,  1037,  3761,  1517,\n",
      "          9130,  2815,  4333,  2005,  2847,  5958,  2044,  8398,  2001,  5496,\n",
      "          1997,  1043, 10626, 11787,  4808,  1999,  8466,  2008,  2596,  2006,\n",
      "          2049,  7248,  1012,   101,   102,   101,  2012,  2260,  1024,  5187,\n",
      "          3286,  3802,  2006,  5958,  2851,  1010,  2004,  5830,  2739,  6125,\n",
      "          3344,  4871,  1997,  8769,  1998, 15615,  8090,  1999, 11334,  1010,\n",
      "          1996,  2343,  1056, 28394,  3064,  1024,  1000,  2122, 24106,  2024,\n",
      "          9841, 17175,  4892,  1996,  3638,  1997,  2577, 12305,  1010,  1998,\n",
      "          1045,  2180,  1005,  1056,  2292,  2008,  4148,  1012,   101,   102,\n",
      "           101,  2074,  3764,  2000,  3099,  5199, 24547,  2480,  1998,  2409,\n",
      "          2032,  2008,  1996,  2510,  2003,  2007,  2032,  2035,  1996,  2126,\n",
      "          1012,   101,   102,   101,  2151,  7669,  1998,  2057,  2097,  7868,\n",
      "          2491,  2021,  1010,  2043,  1996, 29367,  4627,  1010,  1996,  5008,\n",
      "          4627,  1012,   101,   102,   101,  4067,  2017,   999,   101,   102,\n",
      "           101,  2010,  7655,  1000,  2043,  1996, 29367,  4627,  1010,  1996,\n",
      "          5008,  4627,  1010,  1000, 13536,  2653,  2109,  2011,  1037,  5631,\n",
      "          2610,  2708,  1999,  1996,  2397,  4120,  1999,  1996,  5256,  1997,\n",
      "         12925,  1012,   101,   102,   101,  2049,  2224,  2001,  3202, 10033,\n",
      "          2011,  1037,  2898,  9140,  1997,  3633,  1010,  2013,  7862,  2000,\n",
      "          2372,  1997,  6538,  2576,  8008,  1012,   101,   102,   101,  2280,\n",
      "          3580,  2343,  1998,  3653, 17421, 24971,  3537,  9773,  3533,  7226,\n",
      "          2368,  2056,  8398,  2001,  1000,  4214,  2005,  4808,  2114,  2137,\n",
      "          4480,  2076,  1037,  2617,  1997,  3255,  2005,  2061,  2116,  1012,\n",
      "           101,   102,   101,  3191,   102]])\n",
      "index 1 tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.]])\n",
      "seg  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0]])\n",
      "index 3 tensor([[  0,  52,  54,  86,  88, 143, 145, 198, 200, 221, 223, 242, 244, 248,\n",
      "         250, 282, 284, 306, 308, 340, 342]])\n",
      "index 4 tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.]])\n",
      "index 5 [['(CNN) Over and over again in 2018, during an apology tour that took him from the halls of the US Congress to an appearance before the European Parliament, Mark Zuckerberg said Facebook had failed to \"take a broad enough view of our responsibilities.', 'But two years later, Zuckerberg and Facebook are still struggling with their responsibilities and how to handle one of their most famous users: President Donald Trump.', 'Despite Zuckerberg having previously indicated any post that \"incites violence\" would be a line in the sand — even if it came from a politician — Facebook remained silent for hours Friday after Trump was accused of glorifying violence in posts that appeared on its platforms.', 'At 12:53am ET on Friday morning, as cable news networks carried images of fires and destructive protests in Minneapolis, the President tweeted : \"These THUGS are dishonoring the memory of George Floyd, and I won\\'t let that happen.', 'Just spoke to Governor Tim Walz and told him that the Military is with him all the way.', 'Any difficulty and we will assume control but, when the looting starts, the shooting starts.', 'Thank you!', 'His phrase \"when the looting starts, the shooting starts,\" mirrors language used by a Miami police chief in the late 1960s in the wake of riots.', 'Its use was immediately condemned by a wide array of individuals, from historians to members of rival political campaigns.', 'Former Vice President and presumptive Democratic nominee Joe Biden said Trump was \"calling for violence against American citizens during a moment of pain for so many.', 'Read More']]\n",
      "test-----src  tensor([[  101,  1006, 13229,  1007,  2058,  1998,  2058,  2153,  1999,  2760,\n",
      "          1010,  2076,  2019, 12480,  2778,  2008,  2165,  2032,  2013,  1996,\n",
      "          9873,  1997,  1996,  2149,  3519,  2000,  2019,  3311,  2077,  1996,\n",
      "          2647,  3323,  1010,  2928, 16950,  9102,  4059,  2056,  9130,  2018,\n",
      "          3478,  2000,  1000,  2202,  1037,  5041,  2438,  3193,  1997,  2256,\n",
      "         10198,  1012,   101,   102,   101,  2021,  2048,  2086,  2101,  1010,\n",
      "         16950,  9102,  4059,  1998,  9130,  2024,  2145,  8084,  2007,  2037,\n",
      "         10198,  1998,  2129,  2000,  5047,  2028,  1997,  2037,  2087,  3297,\n",
      "          5198,  1024,  2343,  6221,  8398,  1012,   101,   102,   101,  2750,\n",
      "         16950,  9102,  4059,  2383,  3130,  5393,  2151,  2695,  2008,  1000,\n",
      "          4297,  7616,  4808,  1000,  2052,  2022,  1037,  2240,  1999,  1996,\n",
      "          5472,  1517,  2130,  2065,  2009,  2234,  2013,  1037,  3761,  1517,\n",
      "          9130,  2815,  4333,  2005,  2847,  5958,  2044,  8398,  2001,  5496,\n",
      "          1997,  1043, 10626, 11787,  4808,  1999,  8466,  2008,  2596,  2006,\n",
      "          2049,  7248,  1012,   101,   102,   101,  2012,  2260,  1024,  5187,\n",
      "          3286,  3802,  2006,  5958,  2851,  1010,  2004,  5830,  2739,  6125,\n",
      "          3344,  4871,  1997,  8769,  1998, 15615,  8090,  1999, 11334,  1010,\n",
      "          1996,  2343,  1056, 28394,  3064,  1024,  1000,  2122, 24106,  2024,\n",
      "          9841, 17175,  4892,  1996,  3638,  1997,  2577, 12305,  1010,  1998,\n",
      "          1045,  2180,  1005,  1056,  2292,  2008,  4148,  1012,   101,   102,\n",
      "           101,  2074,  3764,  2000,  3099,  5199, 24547,  2480,  1998,  2409,\n",
      "          2032,  2008,  1996,  2510,  2003,  2007,  2032,  2035,  1996,  2126,\n",
      "          1012,   101,   102,   101,  2151,  7669,  1998,  2057,  2097,  7868,\n",
      "          2491,  2021,  1010,  2043,  1996, 29367,  4627,  1010,  1996,  5008,\n",
      "          4627,  1012,   101,   102,   101,  4067,  2017,   999,   101,   102,\n",
      "           101,  2010,  7655,  1000,  2043,  1996, 29367,  4627,  1010,  1996,\n",
      "          5008,  4627,  1010,  1000, 13536,  2653,  2109,  2011,  1037,  5631,\n",
      "          2610,  2708,  1999,  1996,  2397,  4120,  1999,  1996,  5256,  1997,\n",
      "         12925,  1012,   101,   102,   101,  2049,  2224,  2001,  3202, 10033,\n",
      "          2011,  1037,  2898,  9140,  1997,  3633,  1010,  2013,  7862,  2000,\n",
      "          2372,  1997,  6538,  2576,  8008,  1012,   101,   102,   101,  2280,\n",
      "          3580,  2343,  1998,  3653, 17421, 24971,  3537,  9773,  3533,  7226,\n",
      "          2368,  2056,  8398,  2001,  1000,  4214,  2005,  4808,  2114,  2137,\n",
      "          4480,  2076,  1037,  2617,  1997,  3255,  2005,  2061,  2116,  1012,\n",
      "           101,   102,   101,  3191,   102]])\n",
      "test-----index 1 tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.]])\n",
      "test-----seg  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0]])\n",
      "test-----index 3 tensor([[  0,  52,  54,  86,  88, 143, 145, 198, 200, 221, 223, 242, 244, 248,\n",
      "         250, 282, 284, 306, 308, 340, 342]])\n",
      "test-----index 4 tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.]])\n",
      "test-----index 5 [['(CNN) Over and over again in 2018, during an apology tour that took him from the halls of the US Congress to an appearance before the European Parliament, Mark Zuckerberg said Facebook had failed to \"take a broad enough view of our responsibilities.', 'But two years later, Zuckerberg and Facebook are still struggling with their responsibilities and how to handle one of their most famous users: President Donald Trump.', 'Despite Zuckerberg having previously indicated any post that \"incites violence\" would be a line in the sand — even if it came from a politician — Facebook remained silent for hours Friday after Trump was accused of glorifying violence in posts that appeared on its platforms.', 'At 12:53am ET on Friday morning, as cable news networks carried images of fires and destructive protests in Minneapolis, the President tweeted : \"These THUGS are dishonoring the memory of George Floyd, and I won\\'t let that happen.', 'Just spoke to Governor Tim Walz and told him that the Military is with him all the way.', 'Any difficulty and we will assume control but, when the looting starts, the shooting starts.', 'Thank you!', 'His phrase \"when the looting starts, the shooting starts,\" mirrors language used by a Miami police chief in the late 1960s in the wake of riots.', 'Its use was immediately condemned by a wide array of individuals, from historians to members of rival political campaigns.', 'Former Vice President and presumptive Democratic nominee Joe Biden said Trump was \"calling for violence against American citizens during a moment of pain for so many.', 'Read More']]\n",
      "----->check:src tensor([[  101,  1006, 13229,  1007,  2058,  1998,  2058,  2153,  1999,  2760,\n",
      "          1010,  2076,  2019, 12480,  2778,  2008,  2165,  2032,  2013,  1996,\n",
      "          9873,  1997,  1996,  2149,  3519,  2000,  2019,  3311,  2077,  1996,\n",
      "          2647,  3323,  1010,  2928, 16950,  9102,  4059,  2056,  9130,  2018,\n",
      "          3478,  2000,  1000,  2202,  1037,  5041,  2438,  3193,  1997,  2256,\n",
      "         10198,  1012,   101,   102,   101,  2021,  2048,  2086,  2101,  1010,\n",
      "         16950,  9102,  4059,  1998,  9130,  2024,  2145,  8084,  2007,  2037,\n",
      "         10198,  1998,  2129,  2000,  5047,  2028,  1997,  2037,  2087,  3297,\n",
      "          5198,  1024,  2343,  6221,  8398,  1012,   101,   102,   101,  2750,\n",
      "         16950,  9102,  4059,  2383,  3130,  5393,  2151,  2695,  2008,  1000,\n",
      "          4297,  7616,  4808,  1000,  2052,  2022,  1037,  2240,  1999,  1996,\n",
      "          5472,  1517,  2130,  2065,  2009,  2234,  2013,  1037,  3761,  1517,\n",
      "          9130,  2815,  4333,  2005,  2847,  5958,  2044,  8398,  2001,  5496,\n",
      "          1997,  1043, 10626, 11787,  4808,  1999,  8466,  2008,  2596,  2006,\n",
      "          2049,  7248,  1012,   101,   102,   101,  2012,  2260,  1024,  5187,\n",
      "          3286,  3802,  2006,  5958,  2851,  1010,  2004,  5830,  2739,  6125,\n",
      "          3344,  4871,  1997,  8769,  1998, 15615,  8090,  1999, 11334,  1010,\n",
      "          1996,  2343,  1056, 28394,  3064,  1024,  1000,  2122, 24106,  2024,\n",
      "          9841, 17175,  4892,  1996,  3638,  1997,  2577, 12305,  1010,  1998,\n",
      "          1045,  2180,  1005,  1056,  2292,  2008,  4148,  1012,   101,   102,\n",
      "           101,  2074,  3764,  2000,  3099,  5199, 24547,  2480,  1998,  2409,\n",
      "          2032,  2008,  1996,  2510,  2003,  2007,  2032,  2035,  1996,  2126,\n",
      "          1012,   101,   102,   101,  2151,  7669,  1998,  2057,  2097,  7868,\n",
      "          2491,  2021,  1010,  2043,  1996, 29367,  4627,  1010,  1996,  5008,\n",
      "          4627,  1012,   101,   102,   101,  4067,  2017,   999,   101,   102,\n",
      "           101,  2010,  7655,  1000,  2043,  1996, 29367,  4627,  1010,  1996,\n",
      "          5008,  4627,  1010,  1000, 13536,  2653,  2109,  2011,  1037,  5631,\n",
      "          2610,  2708,  1999,  1996,  2397,  4120,  1999,  1996,  5256,  1997,\n",
      "         12925,  1012,   101,   102,   101,  2049,  2224,  2001,  3202, 10033,\n",
      "          2011,  1037,  2898,  9140,  1997,  3633,  1010,  2013,  7862,  2000,\n",
      "          2372,  1997,  6538,  2576,  8008,  1012,   101,   102,   101,  2280,\n",
      "          3580,  2343,  1998,  3653, 17421, 24971,  3537,  9773,  3533,  7226,\n",
      "          2368,  2056,  8398,  2001,  1000,  4214,  2005,  4808,  2114,  2137,\n",
      "          4480,  2076,  1037,  2617,  1997,  3255,  2005,  2061,  2116,  1012,\n",
      "           101,   102,   101,  3191,   102]])\n",
      "----->check:segs tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0]])\n",
      "----->check:mask_src tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.]])\n",
      "@@@@@top_vec [tensor([[[ 0.9428, -1.0221,  0.6432,  ..., -0.4035, -0.4156, -0.0744],\n",
      "         [ 1.0123, -0.4756,  0.0919,  ..., -0.8477, -0.3282, -0.5712],\n",
      "         [ 0.4913, -1.7306, -0.4892,  ..., -2.6768,  0.7740,  0.5169],\n",
      "         ...,\n",
      "         [ 0.4127, -0.2431, -0.2458,  ..., -1.4592, -0.7452,  0.0533],\n",
      "         [-1.7261, -0.7189,  0.3170,  ..., -1.6797, -1.9321,  0.7002],\n",
      "         [ 0.0990,  0.1640, -1.4002,  ..., -0.9223, -1.0730,  1.2704]]]), tensor([[[ 0.6692, -1.2656,  0.7959,  ..., -0.6920, -0.3382,  0.0340],\n",
      "         [ 0.8673, -0.3635,  0.2408,  ..., -0.9723, -0.5989, -0.2918],\n",
      "         [ 0.2069, -1.4967, -0.3551,  ..., -2.9313,  0.8795,  0.5185],\n",
      "         ...,\n",
      "         [-0.0247, -0.2409,  0.0832,  ..., -1.7773, -0.9442, -0.0826],\n",
      "         [-2.2441, -0.5155,  0.2674,  ..., -1.4765, -2.1027,  0.7497],\n",
      "         [-0.0054,  0.0821, -1.3868,  ..., -0.9902, -1.2890,  1.3963]]]), tensor([[[ 0.5165, -1.2409,  0.6925,  ..., -0.8949, -0.4412,  0.1601],\n",
      "         [ 1.1085, -0.5899,  0.2698,  ..., -1.2615, -0.7026, -0.1032],\n",
      "         [ 0.1505, -1.6425, -0.2752,  ..., -3.4467,  0.7871,  0.5625],\n",
      "         ...,\n",
      "         [-0.1726, -0.2182,  0.0125,  ..., -2.0016, -0.9893,  0.3331],\n",
      "         [-2.1515, -0.5330,  0.2396,  ..., -1.9096, -2.3242,  1.0506],\n",
      "         [ 0.0622, -0.0126, -1.3896,  ..., -1.1497, -1.6146,  1.5422]]]), tensor([[[ 0.4749, -1.1774,  0.9502,  ..., -1.1365, -0.7805,  0.1124],\n",
      "         [ 1.3775, -0.4009,  0.6880,  ..., -1.4198, -0.9310, -0.3945],\n",
      "         [ 0.2157, -1.1708, -0.3082,  ..., -3.5321,  0.3902,  0.3235],\n",
      "         ...,\n",
      "         [-0.0213,  0.1665,  0.1629,  ..., -2.3682, -1.0739,  0.2704],\n",
      "         [-1.8939,  0.0127,  0.3176,  ..., -2.1357, -2.4054,  0.8106],\n",
      "         [ 0.3518,  0.2215, -1.0732,  ..., -1.5239, -1.8881,  1.4448]]]), tensor([[[ 0.4726, -1.8151,  1.3016,  ..., -1.3692, -0.6135, -0.0588],\n",
      "         [ 1.4239, -0.3887,  1.0993,  ..., -1.7480, -0.5299, -0.4568],\n",
      "         [ 0.4252, -1.1853, -0.0748,  ..., -3.7659,  1.0124,  0.4635],\n",
      "         ...,\n",
      "         [ 0.1611, -0.2162,  0.3561,  ..., -2.5959, -0.9392,  0.2082],\n",
      "         [-1.9165, -0.2123,  0.5355,  ..., -2.2300, -2.1537,  0.5931],\n",
      "         [ 0.6372,  0.1565, -0.6501,  ..., -1.9397, -1.4523,  1.4532]]]), tensor([[[ 0.3124, -1.5768,  1.7967,  ..., -1.3935, -0.8705, -0.0366],\n",
      "         [ 1.3862, -0.6958,  1.0961,  ..., -2.0925, -0.5505, -0.3313],\n",
      "         [ 0.3690, -1.1191,  0.2577,  ..., -3.7687,  0.7620,  0.5552],\n",
      "         ...,\n",
      "         [ 0.2472, -0.3001,  0.5947,  ..., -2.5358, -1.2237, -0.0706],\n",
      "         [-1.9357, -0.0928,  0.8405,  ..., -2.5578, -1.9534,  0.5917],\n",
      "         [ 0.3033,  0.2297, -0.3869,  ..., -2.1145, -1.6031,  1.1901]]]), tensor([[[ 0.5728, -1.5434,  1.5886,  ..., -1.3988, -0.9430,  0.1938],\n",
      "         [ 1.5090, -0.7111,  1.0385,  ..., -1.9293, -0.4338,  0.1744],\n",
      "         [ 0.4604, -1.1017,  0.2342,  ..., -3.8124,  0.8111,  0.7942],\n",
      "         ...,\n",
      "         [ 0.5079, -0.3495,  0.5739,  ..., -2.7001, -1.3275,  0.1638],\n",
      "         [-1.5892, -0.2996,  0.8089,  ..., -2.6214, -1.8939,  0.6987],\n",
      "         [ 0.7715,  0.1886, -0.3848,  ..., -2.3594, -1.4730,  1.2344]]]), tensor([[[ 0.7492, -1.5032,  1.4991,  ..., -1.5650, -0.5537,  0.3711],\n",
      "         [ 1.4881, -0.5802,  1.0679,  ..., -2.1472, -0.1787,  0.4221],\n",
      "         [ 0.3402, -0.7817,  0.1046,  ..., -3.8356,  1.0845,  0.7469],\n",
      "         ...,\n",
      "         [ 0.7540, -0.0986,  0.5655,  ..., -2.9611, -0.9708,  0.4532],\n",
      "         [-1.7884,  0.0068,  0.9098,  ..., -2.5055, -1.6477,  0.6749],\n",
      "         [ 0.6495,  0.4079, -0.4500,  ..., -2.4272, -1.1619,  1.4771]]]), tensor([[[ 0.8064, -1.8729,  1.4242,  ..., -1.6903, -0.6253,  0.5121],\n",
      "         [ 1.3073, -0.8503,  0.9187,  ..., -2.6838, -0.2646,  0.6620],\n",
      "         [ 0.5623, -0.8728,  0.4746,  ..., -3.6399,  1.1623,  0.9402],\n",
      "         ...,\n",
      "         [ 1.0132, -0.2944,  0.5713,  ..., -3.0502, -1.0767,  0.6832],\n",
      "         [-1.5071, -0.1174,  1.4066,  ..., -2.7542, -1.5418,  0.7740],\n",
      "         [ 0.5296, -0.0105, -0.4783,  ..., -2.5570, -1.0629,  1.8061]]]), tensor([[[ 0.2115, -2.0086,  1.0789,  ..., -1.2858, -0.9744,  0.4934],\n",
      "         [ 0.8741, -0.6228,  0.6375,  ..., -2.4324, -0.5619,  0.4002],\n",
      "         [ 0.0558, -0.8052,  0.2061,  ..., -2.9398,  0.7378,  0.9994],\n",
      "         ...,\n",
      "         [ 0.7696, -0.7669,  0.3378,  ..., -2.8445, -1.3474,  0.6974],\n",
      "         [-1.5601, -0.0066,  1.0369,  ..., -2.1338, -1.5837,  0.5691],\n",
      "         [ 0.4080, -0.0974, -0.4873,  ..., -2.4213, -1.0333,  2.0101]]]), tensor([[[ 0.0982, -1.9400,  1.0151,  ..., -1.0036, -0.9010,  0.7245],\n",
      "         [ 0.9131, -0.7336,  0.6807,  ..., -2.2026, -0.5661,  0.8001],\n",
      "         [ 0.1629, -0.7385,  0.1022,  ..., -2.6915,  1.0023,  0.9747],\n",
      "         ...,\n",
      "         [ 0.8272, -0.6439,  0.1962,  ..., -2.6327, -1.2632,  1.0402],\n",
      "         [-1.7490,  0.3339,  0.9982,  ..., -1.8297, -1.5081,  0.7141],\n",
      "         [ 0.4914, -0.0116, -0.4804,  ..., -2.0394, -0.9984,  2.2195]]]), tensor([[[-0.1481, -1.6131,  1.2614,  ..., -0.6952, -1.2778,  0.3246],\n",
      "         [ 0.9075, -0.5751,  1.1836,  ..., -1.8090, -1.0031,  0.6348],\n",
      "         [-0.0391, -0.5593,  0.4173,  ..., -2.1958,  0.6444,  0.4125],\n",
      "         ...,\n",
      "         [ 0.5050, -0.5822,  0.3001,  ..., -2.2284, -1.6267,  0.5420],\n",
      "         [-1.9826,  0.6611,  1.3098,  ..., -1.4805, -1.6108,  0.0411],\n",
      "         [ 0.4935,  0.0715, -0.0284,  ..., -1.7098, -1.3624,  1.6563]]])]\n",
      "@@@@@second_augment tensor([[-1.3763e-01, -2.2258e-02,  1.3187e-01,  1.2381e-01,  6.5740e-01,\n",
      "          3.2995e-01, -4.8781e-02, -9.0612e-02, -5.4287e-01, -1.5012e-01,\n",
      "         -4.1890e-02, -4.2142e-01, -4.9213e-02, -6.3021e-01, -5.3056e-01,\n",
      "          3.3375e-01, -2.0596e-01, -6.4787e-01,  7.9681e-01,  3.0150e-01,\n",
      "          2.9955e-01,  4.4004e-01,  1.2401e-01,  5.3151e-01, -3.6090e-01,\n",
      "          3.7721e-02,  6.0808e-01,  1.0674e-01, -7.0110e-02, -1.3002e-01,\n",
      "          4.7219e-01,  3.4005e-01,  2.0876e-01,  4.7957e-01, -5.7256e-01,\n",
      "          1.9938e-01, -2.0805e-01,  4.3137e-01,  4.4231e-01,  9.9961e-03,\n",
      "          2.2046e-01,  1.0011e-01, -4.6387e-01,  2.8471e-01,  3.4305e-01,\n",
      "         -6.0833e-01,  1.0472e-01,  2.1165e-01,  8.7315e-02, -5.4803e-01,\n",
      "         -8.0701e-01,  6.7954e-01, -6.5666e-02,  1.4446e-01,  5.6180e-01,\n",
      "          4.5156e-01,  4.2345e-01, -5.4978e-01, -8.4301e-01, -6.4022e-01,\n",
      "         -5.1652e-01, -2.9598e-01, -6.9926e-01,  8.0456e-01, -1.2077e-01,\n",
      "          6.9978e-01, -1.6241e-01, -5.9522e-01,  6.0240e-01, -6.2465e-01,\n",
      "         -8.7321e-02,  3.6934e-01, -1.4076e-01, -6.5023e-01,  4.6262e-02,\n",
      "         -5.5585e-01, -1.7944e-01, -6.8312e-01, -5.4412e-01,  9.2239e-01,\n",
      "         -6.0936e-01, -1.1328e-02,  7.5518e-01, -7.4746e-01,  1.5772e-01,\n",
      "         -2.6881e-02,  5.0150e-02, -3.7297e-01,  4.2475e-01,  6.1109e-02,\n",
      "          3.1961e-02, -2.1740e-02, -3.7569e-01,  3.4641e-01, -4.3229e-01,\n",
      "         -4.0985e-01, -2.2304e-01, -3.7014e-01,  3.2100e-01,  1.4269e-01,\n",
      "          7.9277e-02,  3.7223e-01, -4.4550e-01,  6.9052e-01,  8.0911e-01,\n",
      "         -3.7740e-01,  6.9708e-01, -1.0921e-01, -3.8819e-01,  1.2194e-01,\n",
      "          5.2066e-02, -3.7757e-01,  6.4701e-01,  1.2057e-01,  5.5023e-01,\n",
      "          5.9403e-01,  5.4661e-01, -1.2369e-01, -2.4441e-01, -3.5504e-01,\n",
      "         -4.6085e-01,  5.1887e-01,  3.4658e-01, -2.9402e-01, -7.7686e-01,\n",
      "          5.5502e-01, -5.4224e-01, -1.2476e-02, -4.1169e-01, -4.4141e-02,\n",
      "          1.2851e-01,  1.1582e-01, -5.6354e-01, -8.5214e-02, -4.4224e-01,\n",
      "          3.9244e-01, -5.4992e-01,  1.6702e-01,  4.2548e-01,  3.5643e-01,\n",
      "          8.9763e-02, -1.3331e-01,  4.2436e-01, -1.8340e-01,  1.5156e-01,\n",
      "         -4.4842e-01,  1.1874e-01,  1.3871e-01,  6.8052e-02,  1.0579e-01,\n",
      "          1.0210e-01, -3.9824e-01,  3.8822e-01, -2.7494e-01,  1.8553e-01,\n",
      "         -3.7489e-01,  7.8841e-01, -7.4640e-01,  9.0619e-01,  3.0276e-01,\n",
      "         -8.2456e-01, -1.9590e-02, -3.0948e-02,  1.5298e-01, -5.2777e-01,\n",
      "          8.1631e-02, -3.6885e-01, -1.9788e-01, -7.5741e-03, -4.7539e-01,\n",
      "          1.7938e-03, -2.1130e-01,  3.7536e-01,  7.1138e-01,  1.9344e-01,\n",
      "          1.5555e-01, -1.7409e-01,  2.7663e-01,  5.4059e-01,  6.2962e-01,\n",
      "          1.5612e-01, -1.9601e-01, -2.1839e-01,  5.1340e-01, -6.0681e-01,\n",
      "         -1.7955e-01,  8.3551e-01, -7.1606e-01,  5.0904e-01,  7.4200e-01,\n",
      "         -3.2370e-01, -2.0912e-01,  8.0260e-01,  1.4138e-01,  6.7788e-01,\n",
      "          3.4227e-01,  2.7380e-01, -5.9894e-01, -7.8542e-02, -2.0202e-01,\n",
      "         -5.8630e-04, -5.5910e-01,  2.6399e-01,  2.2597e-01, -3.0477e-01,\n",
      "          4.8105e-01, -3.3426e-01, -7.8657e-01, -6.5618e-01,  4.0096e-01,\n",
      "         -2.9314e-02,  1.2612e-02, -2.0665e-01,  2.2820e-01, -7.6160e-01,\n",
      "         -8.4539e-02, -1.0945e-01,  4.3533e-01,  5.8645e-01, -6.2593e-01,\n",
      "         -6.0610e-01,  1.7476e-01, -1.1262e-01, -8.3173e-01, -1.6848e-01,\n",
      "          4.7601e-02,  1.6469e-01,  7.1797e-02, -2.5886e-01,  1.7228e-01,\n",
      "         -3.0087e-01,  1.8589e-01,  1.9391e-02, -7.0563e-01, -5.8408e-01,\n",
      "          4.6167e-01, -7.0959e-02, -5.2072e-01, -6.1207e-01, -4.0117e-01,\n",
      "         -1.1900e-01,  5.4072e-03, -1.4205e-01,  2.1480e-01, -2.7251e-01,\n",
      "         -3.6595e-01,  2.7789e-01,  7.7665e-01, -8.0693e-03,  7.9212e-03,\n",
      "          7.2180e-01,  5.4270e-02, -3.7991e-01,  4.4071e-01,  3.0267e-01,\n",
      "         -2.1806e-01, -1.3090e-01, -2.6043e-02, -2.9271e-02,  5.0165e-01,\n",
      "         -1.4235e-01,  5.2575e-02, -5.1243e-01,  7.6074e-01, -2.3817e-02,\n",
      "          5.1814e-01, -3.5825e-01,  5.2335e-01, -5.6492e-01, -2.5242e-01,\n",
      "         -7.0379e-01, -5.1951e-02, -8.1548e-02, -1.0451e-02, -1.5020e-01,\n",
      "         -5.4850e-01,  4.5684e-01,  2.7259e-01, -9.4223e-01, -6.6294e-01,\n",
      "         -4.1415e-01,  4.4494e-01, -4.3551e-01,  1.5708e-01, -3.7462e-01,\n",
      "          5.4895e-01, -9.3111e-01, -6.6157e-01, -3.0867e-03,  2.1982e-01,\n",
      "          5.1325e-01,  1.0814e-01,  1.8042e-01,  3.8329e-01,  9.7539e-02,\n",
      "          5.8371e-01, -3.4063e-01,  4.1166e-01, -7.1902e-01,  5.0785e-01,\n",
      "         -3.3493e-01, -3.0066e-01, -4.5498e-01,  1.0978e-01, -4.5647e-01,\n",
      "         -3.4749e-01, -2.8410e-01,  7.6057e-01, -5.8712e-01,  2.0947e-01,\n",
      "          2.6944e-01, -2.4296e-01,  4.4250e-01, -3.4940e-01,  2.5598e-01,\n",
      "          2.9311e-01, -2.3469e-01, -3.5033e-01, -4.1484e-01, -1.1518e-01,\n",
      "          2.6887e-01,  2.6019e-01,  7.4744e-01, -4.1412e-01, -5.3277e-01,\n",
      "          5.3900e-01,  4.2886e-01, -5.8416e-01, -1.0656e-01, -1.0232e-01,\n",
      "          5.5093e-01, -5.3178e-01, -2.5498e-01,  1.0484e-01, -5.6683e-01,\n",
      "         -1.3956e-01,  4.3643e-01,  4.2651e-02,  1.6893e-01,  6.0251e-01,\n",
      "          3.0768e-01,  1.3215e-01, -1.1026e-01,  8.7980e-01, -1.1872e-01,\n",
      "          1.7438e-01, -5.9698e-01,  3.4453e-01,  4.7171e-01,  5.9492e-01,\n",
      "         -3.8185e-01, -6.2186e-01, -4.1700e-01,  1.3850e-01, -3.0163e-01,\n",
      "          3.4550e-01, -1.4793e-03,  3.4875e-01, -1.9840e-01, -5.3302e-02,\n",
      "         -4.5215e-01, -5.7991e-01, -5.1983e-01, -2.0471e-01,  7.1060e-01,\n",
      "         -2.4645e-01, -7.0019e-01,  3.2503e-01, -1.5685e-01,  3.8575e-01,\n",
      "         -6.8975e-01,  1.4300e-01, -1.0491e-01,  7.3766e-01, -1.0073e-01,\n",
      "         -5.9394e-01,  1.0002e-01, -2.5464e-01,  8.8239e-01, -4.4982e-01,\n",
      "          3.4129e-01, -6.3353e-01,  3.5694e-01, -1.6471e-01, -4.8991e-01,\n",
      "          1.1121e-01,  1.9602e-01, -2.0999e-01,  5.3466e-01,  1.8864e-01,\n",
      "         -2.4355e-01, -2.0016e-02,  4.2221e-01, -5.0517e-01,  1.5029e-01,\n",
      "          4.9586e-01,  2.7337e-01,  4.1050e-01,  3.4980e-01,  2.1663e-01,\n",
      "         -6.1761e-01,  1.0463e-01, -9.2930e-01,  8.7147e-02, -3.0428e-01,\n",
      "         -2.4873e-01,  4.9223e-02,  6.1546e-01,  5.2097e-01,  7.1743e-02,\n",
      "         -5.2919e-01, -1.5920e-01, -2.2190e-01,  1.2166e-02, -3.4507e-01,\n",
      "         -2.3521e-01,  3.8543e-01, -4.3203e-01, -9.4528e-02,  8.9104e-01,\n",
      "         -1.8526e-02, -7.9263e-01,  6.9249e-01, -5.0825e-01, -6.2631e-01,\n",
      "          2.3463e-01, -3.8438e-02,  1.2812e-01, -3.4156e-01,  8.4767e-01,\n",
      "          2.6582e-01,  1.0797e-01,  5.4924e-01,  2.3647e-01,  2.3388e-01,\n",
      "          4.8136e-01,  4.7939e-01, -3.2949e-01,  1.2287e-01, -3.8879e-01,\n",
      "         -1.7197e-01,  3.9972e-01,  8.9306e-01,  4.8864e-01, -1.5062e-01,\n",
      "         -3.6574e-01,  6.0109e-01, -3.3774e-01, -2.7780e-01, -6.0397e-01,\n",
      "         -7.1056e-01, -4.2388e-01,  7.4412e-01, -2.5665e-01,  1.6475e-01,\n",
      "          3.4171e-02, -3.5819e-01,  3.6765e-02,  7.2413e-02, -6.6103e-01,\n",
      "         -4.9783e-01, -1.8436e-01, -1.8091e-01,  4.0348e-01,  5.3303e-01,\n",
      "         -5.8878e-01, -8.3434e-01, -1.3883e-01,  1.2027e-01,  5.1529e-01,\n",
      "          5.3331e-01, -4.2548e-01, -5.2125e-02,  9.0430e-01, -8.1040e-01,\n",
      "          6.5163e-01, -5.7353e-03, -7.5640e-02, -1.7954e-01, -4.3089e-01,\n",
      "          7.4816e-01,  3.0857e-01,  6.2197e-01,  1.4283e-01, -2.1328e-01,\n",
      "         -6.8431e-01, -8.7341e-02, -3.2781e-01, -7.0649e-01, -7.5675e-01,\n",
      "          6.4710e-01, -2.6425e-02,  7.7514e-01, -1.6659e-01,  6.0696e-01,\n",
      "          6.2241e-01,  1.3475e-01,  8.4923e-02, -3.1559e-01,  2.5994e-01,\n",
      "         -1.2634e-01, -3.8146e-01, -4.1696e-01,  2.1094e-01,  6.4943e-01,\n",
      "         -1.0232e-01, -6.7943e-01,  2.2563e-01,  2.1779e-01, -7.0126e-02,\n",
      "         -2.0449e-01,  3.7325e-01,  3.9012e-01,  2.5650e-01, -3.3272e-01,\n",
      "         -6.8736e-02,  2.8310e-01,  1.1413e-01,  4.2647e-01,  2.4188e-01,\n",
      "         -6.3870e-01,  5.1975e-01,  7.6801e-01, -4.7669e-01, -5.6858e-01,\n",
      "         -3.6729e-02,  2.6047e-01, -2.4849e-03, -4.4097e-01,  6.8775e-01,\n",
      "         -1.0895e-01,  6.7677e-01, -8.1765e-01,  5.9303e-01, -3.7041e-01,\n",
      "          3.1205e-01,  7.3197e-01,  1.1418e-01,  7.7504e-02,  2.5785e-01,\n",
      "          1.1453e-01,  7.8014e-02, -7.7173e-01,  6.7930e-01,  3.8251e-03,\n",
      "         -1.0916e-01, -8.4054e-01, -6.1248e-01, -3.0563e-01,  2.3356e-01,\n",
      "          3.4422e-01, -3.3198e-01, -4.2898e-02,  1.4106e-01,  3.0856e-01,\n",
      "         -2.5778e-01,  3.0977e-01,  5.2859e-01,  4.2901e-01,  1.1405e-01,\n",
      "          3.4763e-02, -1.3504e-01, -5.9955e-01,  3.2442e-01,  2.0717e-02,\n",
      "         -3.9998e-01, -6.0657e-01,  2.4247e-02, -4.7224e-01, -1.6868e-02,\n",
      "          8.4722e-01,  2.6434e-01,  3.4197e-02,  8.4250e-02,  3.9864e-01,\n",
      "          4.0327e-01, -3.3132e-01, -3.9821e-01, -2.8717e-01,  1.8992e-01,\n",
      "          5.8127e-01,  8.1954e-01,  4.5317e-03, -5.9954e-01, -4.6415e-01,\n",
      "         -3.2238e-01,  8.0489e-01, -5.2633e-01, -9.1573e-02,  3.7462e-02,\n",
      "          2.8001e-02, -1.8355e-01, -8.4404e-01,  4.3027e-01, -4.6241e-01,\n",
      "         -3.0825e-01, -5.9820e-01,  2.4225e-01,  5.1399e-01,  2.0314e-01,\n",
      "          1.2194e-01, -1.9961e-01,  7.5043e-01, -4.3048e-01, -1.8552e-01,\n",
      "         -1.4612e-01, -4.2893e-02,  7.7254e-01,  1.5950e-01,  7.1895e-01,\n",
      "         -8.1597e-01, -9.6405e-02,  1.6373e-01,  4.4259e-01,  8.7521e-01,\n",
      "         -1.6918e-01,  7.2870e-01, -6.8784e-01,  3.5661e-01, -6.7353e-01,\n",
      "         -6.5746e-01,  7.2793e-02,  1.6972e-02,  4.6179e-01,  7.8171e-01,\n",
      "         -2.7454e-01, -1.0234e-01,  1.8893e-01, -3.4525e-01,  9.2286e-01,\n",
      "          4.7580e-01,  4.6738e-01,  7.5324e-01, -5.0282e-02,  7.8009e-01,\n",
      "         -8.2286e-01, -2.6784e-01,  8.0438e-02,  1.3404e-01, -6.3337e-01,\n",
      "          1.4721e-01, -1.9698e-01,  4.1683e-01, -4.0355e-01,  6.8000e-01,\n",
      "         -2.7094e-01,  1.4035e-01, -5.8443e-01,  3.3017e-01, -2.1370e-01,\n",
      "         -6.3375e-01,  2.0076e-01,  5.3611e-01, -5.4417e-01,  6.3801e-01,\n",
      "          8.5216e-02,  6.0619e-02, -3.4662e-01,  2.7828e-01, -1.7899e-01,\n",
      "         -2.7837e-01,  4.2795e-01, -1.0962e-01, -1.7070e-01, -4.1338e-01,\n",
      "         -1.2285e-02, -5.0270e-01,  1.2178e-01,  1.6744e-01,  1.2656e-01,\n",
      "         -5.3946e-01,  2.0417e-01,  5.5211e-01, -6.3681e-01,  1.7771e-01,\n",
      "         -6.7682e-01,  2.7490e-01, -2.5093e-01, -3.0819e-01,  1.2316e-01,\n",
      "          5.0731e-01, -2.6312e-01, -1.7550e-01,  6.1530e-01,  5.7089e-02,\n",
      "          3.5945e-02,  6.8713e-01,  4.9439e-01,  5.6682e-01,  4.2865e-01,\n",
      "         -7.5932e-01, -4.8139e-01,  3.0273e-01, -7.6109e-03, -3.2233e-01,\n",
      "          5.9681e-01, -3.1865e-01,  3.5872e-01,  1.3894e-01, -2.7099e-01,\n",
      "         -5.3099e-01,  2.8692e-01, -3.6490e-01, -5.3318e-01,  5.2250e-01,\n",
      "         -3.0487e-01,  4.9686e-01, -4.8967e-01, -8.5804e-02,  3.1345e-01,\n",
      "         -4.9762e-01,  3.5370e-01, -2.8132e-01,  6.6011e-01, -4.8579e-02,\n",
      "          3.5809e-01, -3.0036e-01,  5.7269e-02,  2.0102e-01,  2.6052e-01,\n",
      "          7.1001e-01, -7.7328e-01, -3.5708e-01, -4.1782e-01,  4.9272e-01,\n",
      "          4.7822e-01, -1.5500e-01, -3.0384e-01, -2.8447e-01,  2.6324e-01,\n",
      "         -3.6374e-01, -2.1843e-02, -2.3321e-01,  6.3561e-01,  4.0147e-01,\n",
      "         -3.9899e-01,  6.5922e-01,  3.2437e-01,  7.6183e-01, -5.2837e-01,\n",
      "          4.7100e-01,  3.4366e-01,  4.6842e-01,  5.5830e-01, -7.5650e-01,\n",
      "         -2.7779e-01, -2.8835e-01,  6.1263e-01,  5.6796e-01, -7.9285e-01,\n",
      "          8.0573e-02, -5.1478e-01,  4.0543e-01, -3.5554e-02,  1.2567e-01,\n",
      "         -5.5508e-01,  7.7653e-01, -4.3150e-01, -3.9933e-01, -1.9720e-01,\n",
      "         -5.0073e-01, -2.5611e-01, -5.7549e-01,  4.0290e-02,  7.8071e-01,\n",
      "          4.0168e-01,  6.2051e-01,  9.1739e-02]])\n",
      "@@@@@@@@ top_vec size 12\n",
      "@@@@@second_augment size tensor([[-1.3763e-01, -2.2258e-02,  1.3187e-01,  1.2381e-01,  6.5740e-01,\n",
      "          3.2995e-01, -4.8781e-02, -9.0612e-02, -5.4287e-01, -1.5012e-01,\n",
      "         -4.1890e-02, -4.2142e-01, -4.9213e-02, -6.3021e-01, -5.3056e-01,\n",
      "          3.3375e-01, -2.0596e-01, -6.4787e-01,  7.9681e-01,  3.0150e-01,\n",
      "          2.9955e-01,  4.4004e-01,  1.2401e-01,  5.3151e-01, -3.6090e-01,\n",
      "          3.7721e-02,  6.0808e-01,  1.0674e-01, -7.0110e-02, -1.3002e-01,\n",
      "          4.7219e-01,  3.4005e-01,  2.0876e-01,  4.7957e-01, -5.7256e-01,\n",
      "          1.9938e-01, -2.0805e-01,  4.3137e-01,  4.4231e-01,  9.9961e-03,\n",
      "          2.2046e-01,  1.0011e-01, -4.6387e-01,  2.8471e-01,  3.4305e-01,\n",
      "         -6.0833e-01,  1.0472e-01,  2.1165e-01,  8.7315e-02, -5.4803e-01,\n",
      "         -8.0701e-01,  6.7954e-01, -6.5666e-02,  1.4446e-01,  5.6180e-01,\n",
      "          4.5156e-01,  4.2345e-01, -5.4978e-01, -8.4301e-01, -6.4022e-01,\n",
      "         -5.1652e-01, -2.9598e-01, -6.9926e-01,  8.0456e-01, -1.2077e-01,\n",
      "          6.9978e-01, -1.6241e-01, -5.9522e-01,  6.0240e-01, -6.2465e-01,\n",
      "         -8.7321e-02,  3.6934e-01, -1.4076e-01, -6.5023e-01,  4.6262e-02,\n",
      "         -5.5585e-01, -1.7944e-01, -6.8312e-01, -5.4412e-01,  9.2239e-01,\n",
      "         -6.0936e-01, -1.1328e-02,  7.5518e-01, -7.4746e-01,  1.5772e-01,\n",
      "         -2.6881e-02,  5.0150e-02, -3.7297e-01,  4.2475e-01,  6.1109e-02,\n",
      "          3.1961e-02, -2.1740e-02, -3.7569e-01,  3.4641e-01, -4.3229e-01,\n",
      "         -4.0985e-01, -2.2304e-01, -3.7014e-01,  3.2100e-01,  1.4269e-01,\n",
      "          7.9277e-02,  3.7223e-01, -4.4550e-01,  6.9052e-01,  8.0911e-01,\n",
      "         -3.7740e-01,  6.9708e-01, -1.0921e-01, -3.8819e-01,  1.2194e-01,\n",
      "          5.2066e-02, -3.7757e-01,  6.4701e-01,  1.2057e-01,  5.5023e-01,\n",
      "          5.9403e-01,  5.4661e-01, -1.2369e-01, -2.4441e-01, -3.5504e-01,\n",
      "         -4.6085e-01,  5.1887e-01,  3.4658e-01, -2.9402e-01, -7.7686e-01,\n",
      "          5.5502e-01, -5.4224e-01, -1.2476e-02, -4.1169e-01, -4.4141e-02,\n",
      "          1.2851e-01,  1.1582e-01, -5.6354e-01, -8.5214e-02, -4.4224e-01,\n",
      "          3.9244e-01, -5.4992e-01,  1.6702e-01,  4.2548e-01,  3.5643e-01,\n",
      "          8.9763e-02, -1.3331e-01,  4.2436e-01, -1.8340e-01,  1.5156e-01,\n",
      "         -4.4842e-01,  1.1874e-01,  1.3871e-01,  6.8052e-02,  1.0579e-01,\n",
      "          1.0210e-01, -3.9824e-01,  3.8822e-01, -2.7494e-01,  1.8553e-01,\n",
      "         -3.7489e-01,  7.8841e-01, -7.4640e-01,  9.0619e-01,  3.0276e-01,\n",
      "         -8.2456e-01, -1.9590e-02, -3.0948e-02,  1.5298e-01, -5.2777e-01,\n",
      "          8.1631e-02, -3.6885e-01, -1.9788e-01, -7.5741e-03, -4.7539e-01,\n",
      "          1.7938e-03, -2.1130e-01,  3.7536e-01,  7.1138e-01,  1.9344e-01,\n",
      "          1.5555e-01, -1.7409e-01,  2.7663e-01,  5.4059e-01,  6.2962e-01,\n",
      "          1.5612e-01, -1.9601e-01, -2.1839e-01,  5.1340e-01, -6.0681e-01,\n",
      "         -1.7955e-01,  8.3551e-01, -7.1606e-01,  5.0904e-01,  7.4200e-01,\n",
      "         -3.2370e-01, -2.0912e-01,  8.0260e-01,  1.4138e-01,  6.7788e-01,\n",
      "          3.4227e-01,  2.7380e-01, -5.9894e-01, -7.8542e-02, -2.0202e-01,\n",
      "         -5.8630e-04, -5.5910e-01,  2.6399e-01,  2.2597e-01, -3.0477e-01,\n",
      "          4.8105e-01, -3.3426e-01, -7.8657e-01, -6.5618e-01,  4.0096e-01,\n",
      "         -2.9314e-02,  1.2612e-02, -2.0665e-01,  2.2820e-01, -7.6160e-01,\n",
      "         -8.4539e-02, -1.0945e-01,  4.3533e-01,  5.8645e-01, -6.2593e-01,\n",
      "         -6.0610e-01,  1.7476e-01, -1.1262e-01, -8.3173e-01, -1.6848e-01,\n",
      "          4.7601e-02,  1.6469e-01,  7.1797e-02, -2.5886e-01,  1.7228e-01,\n",
      "         -3.0087e-01,  1.8589e-01,  1.9391e-02, -7.0563e-01, -5.8408e-01,\n",
      "          4.6167e-01, -7.0959e-02, -5.2072e-01, -6.1207e-01, -4.0117e-01,\n",
      "         -1.1900e-01,  5.4072e-03, -1.4205e-01,  2.1480e-01, -2.7251e-01,\n",
      "         -3.6595e-01,  2.7789e-01,  7.7665e-01, -8.0693e-03,  7.9212e-03,\n",
      "          7.2180e-01,  5.4270e-02, -3.7991e-01,  4.4071e-01,  3.0267e-01,\n",
      "         -2.1806e-01, -1.3090e-01, -2.6043e-02, -2.9271e-02,  5.0165e-01,\n",
      "         -1.4235e-01,  5.2575e-02, -5.1243e-01,  7.6074e-01, -2.3817e-02,\n",
      "          5.1814e-01, -3.5825e-01,  5.2335e-01, -5.6492e-01, -2.5242e-01,\n",
      "         -7.0379e-01, -5.1951e-02, -8.1548e-02, -1.0451e-02, -1.5020e-01,\n",
      "         -5.4850e-01,  4.5684e-01,  2.7259e-01, -9.4223e-01, -6.6294e-01,\n",
      "         -4.1415e-01,  4.4494e-01, -4.3551e-01,  1.5708e-01, -3.7462e-01,\n",
      "          5.4895e-01, -9.3111e-01, -6.6157e-01, -3.0867e-03,  2.1982e-01,\n",
      "          5.1325e-01,  1.0814e-01,  1.8042e-01,  3.8329e-01,  9.7539e-02,\n",
      "          5.8371e-01, -3.4063e-01,  4.1166e-01, -7.1902e-01,  5.0785e-01,\n",
      "         -3.3493e-01, -3.0066e-01, -4.5498e-01,  1.0978e-01, -4.5647e-01,\n",
      "         -3.4749e-01, -2.8410e-01,  7.6057e-01, -5.8712e-01,  2.0947e-01,\n",
      "          2.6944e-01, -2.4296e-01,  4.4250e-01, -3.4940e-01,  2.5598e-01,\n",
      "          2.9311e-01, -2.3469e-01, -3.5033e-01, -4.1484e-01, -1.1518e-01,\n",
      "          2.6887e-01,  2.6019e-01,  7.4744e-01, -4.1412e-01, -5.3277e-01,\n",
      "          5.3900e-01,  4.2886e-01, -5.8416e-01, -1.0656e-01, -1.0232e-01,\n",
      "          5.5093e-01, -5.3178e-01, -2.5498e-01,  1.0484e-01, -5.6683e-01,\n",
      "         -1.3956e-01,  4.3643e-01,  4.2651e-02,  1.6893e-01,  6.0251e-01,\n",
      "          3.0768e-01,  1.3215e-01, -1.1026e-01,  8.7980e-01, -1.1872e-01,\n",
      "          1.7438e-01, -5.9698e-01,  3.4453e-01,  4.7171e-01,  5.9492e-01,\n",
      "         -3.8185e-01, -6.2186e-01, -4.1700e-01,  1.3850e-01, -3.0163e-01,\n",
      "          3.4550e-01, -1.4793e-03,  3.4875e-01, -1.9840e-01, -5.3302e-02,\n",
      "         -4.5215e-01, -5.7991e-01, -5.1983e-01, -2.0471e-01,  7.1060e-01,\n",
      "         -2.4645e-01, -7.0019e-01,  3.2503e-01, -1.5685e-01,  3.8575e-01,\n",
      "         -6.8975e-01,  1.4300e-01, -1.0491e-01,  7.3766e-01, -1.0073e-01,\n",
      "         -5.9394e-01,  1.0002e-01, -2.5464e-01,  8.8239e-01, -4.4982e-01,\n",
      "          3.4129e-01, -6.3353e-01,  3.5694e-01, -1.6471e-01, -4.8991e-01,\n",
      "          1.1121e-01,  1.9602e-01, -2.0999e-01,  5.3466e-01,  1.8864e-01,\n",
      "         -2.4355e-01, -2.0016e-02,  4.2221e-01, -5.0517e-01,  1.5029e-01,\n",
      "          4.9586e-01,  2.7337e-01,  4.1050e-01,  3.4980e-01,  2.1663e-01,\n",
      "         -6.1761e-01,  1.0463e-01, -9.2930e-01,  8.7147e-02, -3.0428e-01,\n",
      "         -2.4873e-01,  4.9223e-02,  6.1546e-01,  5.2097e-01,  7.1743e-02,\n",
      "         -5.2919e-01, -1.5920e-01, -2.2190e-01,  1.2166e-02, -3.4507e-01,\n",
      "         -2.3521e-01,  3.8543e-01, -4.3203e-01, -9.4528e-02,  8.9104e-01,\n",
      "         -1.8526e-02, -7.9263e-01,  6.9249e-01, -5.0825e-01, -6.2631e-01,\n",
      "          2.3463e-01, -3.8438e-02,  1.2812e-01, -3.4156e-01,  8.4767e-01,\n",
      "          2.6582e-01,  1.0797e-01,  5.4924e-01,  2.3647e-01,  2.3388e-01,\n",
      "          4.8136e-01,  4.7939e-01, -3.2949e-01,  1.2287e-01, -3.8879e-01,\n",
      "         -1.7197e-01,  3.9972e-01,  8.9306e-01,  4.8864e-01, -1.5062e-01,\n",
      "         -3.6574e-01,  6.0109e-01, -3.3774e-01, -2.7780e-01, -6.0397e-01,\n",
      "         -7.1056e-01, -4.2388e-01,  7.4412e-01, -2.5665e-01,  1.6475e-01,\n",
      "          3.4171e-02, -3.5819e-01,  3.6765e-02,  7.2413e-02, -6.6103e-01,\n",
      "         -4.9783e-01, -1.8436e-01, -1.8091e-01,  4.0348e-01,  5.3303e-01,\n",
      "         -5.8878e-01, -8.3434e-01, -1.3883e-01,  1.2027e-01,  5.1529e-01,\n",
      "          5.3331e-01, -4.2548e-01, -5.2125e-02,  9.0430e-01, -8.1040e-01,\n",
      "          6.5163e-01, -5.7353e-03, -7.5640e-02, -1.7954e-01, -4.3089e-01,\n",
      "          7.4816e-01,  3.0857e-01,  6.2197e-01,  1.4283e-01, -2.1328e-01,\n",
      "         -6.8431e-01, -8.7341e-02, -3.2781e-01, -7.0649e-01, -7.5675e-01,\n",
      "          6.4710e-01, -2.6425e-02,  7.7514e-01, -1.6659e-01,  6.0696e-01,\n",
      "          6.2241e-01,  1.3475e-01,  8.4923e-02, -3.1559e-01,  2.5994e-01,\n",
      "         -1.2634e-01, -3.8146e-01, -4.1696e-01,  2.1094e-01,  6.4943e-01,\n",
      "         -1.0232e-01, -6.7943e-01,  2.2563e-01,  2.1779e-01, -7.0126e-02,\n",
      "         -2.0449e-01,  3.7325e-01,  3.9012e-01,  2.5650e-01, -3.3272e-01,\n",
      "         -6.8736e-02,  2.8310e-01,  1.1413e-01,  4.2647e-01,  2.4188e-01,\n",
      "         -6.3870e-01,  5.1975e-01,  7.6801e-01, -4.7669e-01, -5.6858e-01,\n",
      "         -3.6729e-02,  2.6047e-01, -2.4849e-03, -4.4097e-01,  6.8775e-01,\n",
      "         -1.0895e-01,  6.7677e-01, -8.1765e-01,  5.9303e-01, -3.7041e-01,\n",
      "          3.1205e-01,  7.3197e-01,  1.1418e-01,  7.7504e-02,  2.5785e-01,\n",
      "          1.1453e-01,  7.8014e-02, -7.7173e-01,  6.7930e-01,  3.8251e-03,\n",
      "         -1.0916e-01, -8.4054e-01, -6.1248e-01, -3.0563e-01,  2.3356e-01,\n",
      "          3.4422e-01, -3.3198e-01, -4.2898e-02,  1.4106e-01,  3.0856e-01,\n",
      "         -2.5778e-01,  3.0977e-01,  5.2859e-01,  4.2901e-01,  1.1405e-01,\n",
      "          3.4763e-02, -1.3504e-01, -5.9955e-01,  3.2442e-01,  2.0717e-02,\n",
      "         -3.9998e-01, -6.0657e-01,  2.4247e-02, -4.7224e-01, -1.6868e-02,\n",
      "          8.4722e-01,  2.6434e-01,  3.4197e-02,  8.4250e-02,  3.9864e-01,\n",
      "          4.0327e-01, -3.3132e-01, -3.9821e-01, -2.8717e-01,  1.8992e-01,\n",
      "          5.8127e-01,  8.1954e-01,  4.5317e-03, -5.9954e-01, -4.6415e-01,\n",
      "         -3.2238e-01,  8.0489e-01, -5.2633e-01, -9.1573e-02,  3.7462e-02,\n",
      "          2.8001e-02, -1.8355e-01, -8.4404e-01,  4.3027e-01, -4.6241e-01,\n",
      "         -3.0825e-01, -5.9820e-01,  2.4225e-01,  5.1399e-01,  2.0314e-01,\n",
      "          1.2194e-01, -1.9961e-01,  7.5043e-01, -4.3048e-01, -1.8552e-01,\n",
      "         -1.4612e-01, -4.2893e-02,  7.7254e-01,  1.5950e-01,  7.1895e-01,\n",
      "         -8.1597e-01, -9.6405e-02,  1.6373e-01,  4.4259e-01,  8.7521e-01,\n",
      "         -1.6918e-01,  7.2870e-01, -6.8784e-01,  3.5661e-01, -6.7353e-01,\n",
      "         -6.5746e-01,  7.2793e-02,  1.6972e-02,  4.6179e-01,  7.8171e-01,\n",
      "         -2.7454e-01, -1.0234e-01,  1.8893e-01, -3.4525e-01,  9.2286e-01,\n",
      "          4.7580e-01,  4.6738e-01,  7.5324e-01, -5.0282e-02,  7.8009e-01,\n",
      "         -8.2286e-01, -2.6784e-01,  8.0438e-02,  1.3404e-01, -6.3337e-01,\n",
      "          1.4721e-01, -1.9698e-01,  4.1683e-01, -4.0355e-01,  6.8000e-01,\n",
      "         -2.7094e-01,  1.4035e-01, -5.8443e-01,  3.3017e-01, -2.1370e-01,\n",
      "         -6.3375e-01,  2.0076e-01,  5.3611e-01, -5.4417e-01,  6.3801e-01,\n",
      "          8.5216e-02,  6.0619e-02, -3.4662e-01,  2.7828e-01, -1.7899e-01,\n",
      "         -2.7837e-01,  4.2795e-01, -1.0962e-01, -1.7070e-01, -4.1338e-01,\n",
      "         -1.2285e-02, -5.0270e-01,  1.2178e-01,  1.6744e-01,  1.2656e-01,\n",
      "         -5.3946e-01,  2.0417e-01,  5.5211e-01, -6.3681e-01,  1.7771e-01,\n",
      "         -6.7682e-01,  2.7490e-01, -2.5093e-01, -3.0819e-01,  1.2316e-01,\n",
      "          5.0731e-01, -2.6312e-01, -1.7550e-01,  6.1530e-01,  5.7089e-02,\n",
      "          3.5945e-02,  6.8713e-01,  4.9439e-01,  5.6682e-01,  4.2865e-01,\n",
      "         -7.5932e-01, -4.8139e-01,  3.0273e-01, -7.6109e-03, -3.2233e-01,\n",
      "          5.9681e-01, -3.1865e-01,  3.5872e-01,  1.3894e-01, -2.7099e-01,\n",
      "         -5.3099e-01,  2.8692e-01, -3.6490e-01, -5.3318e-01,  5.2250e-01,\n",
      "         -3.0487e-01,  4.9686e-01, -4.8967e-01, -8.5804e-02,  3.1345e-01,\n",
      "         -4.9762e-01,  3.5370e-01, -2.8132e-01,  6.6011e-01, -4.8579e-02,\n",
      "          3.5809e-01, -3.0036e-01,  5.7269e-02,  2.0102e-01,  2.6052e-01,\n",
      "          7.1001e-01, -7.7328e-01, -3.5708e-01, -4.1782e-01,  4.9272e-01,\n",
      "          4.7822e-01, -1.5500e-01, -3.0384e-01, -2.8447e-01,  2.6324e-01,\n",
      "         -3.6374e-01, -2.1843e-02, -2.3321e-01,  6.3561e-01,  4.0147e-01,\n",
      "         -3.9899e-01,  6.5922e-01,  3.2437e-01,  7.6183e-01, -5.2837e-01,\n",
      "          4.7100e-01,  3.4366e-01,  4.6842e-01,  5.5830e-01, -7.5650e-01,\n",
      "         -2.7779e-01, -2.8835e-01,  6.1263e-01,  5.6796e-01, -7.9285e-01,\n",
      "          8.0573e-02, -5.1478e-01,  4.0543e-01, -3.5554e-02,  1.2567e-01,\n",
      "         -5.5508e-01,  7.7653e-01, -4.3150e-01, -3.9933e-01, -1.9720e-01,\n",
      "         -5.0073e-01, -2.5611e-01, -5.7549e-01,  4.0290e-02,  7.8071e-01,\n",
      "          4.0168e-01,  6.2051e-01,  9.1739e-02]])\n",
      "top_vec is not empty\n",
      "EXTTTTT len top_vec 12\n",
      "EXTTTTT[0]len top_vec  1\n",
      "EXTTTTT[1] len top_vec  1\n",
      "type of top_vec <class 'list'>\n",
      "type of[0] top_vec <class 'torch.Tensor'>\n",
      "type of[1] top_vec <class 'torch.Tensor'>\n",
      "len[0] [0] top_vec size  345\n",
      "len[1] [0] top_vec size  345\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "arange(): argument 'end' (position 1) must be Number, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/camilleko/NLP_summarization/ext_summarize.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m input_fp \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39minput.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m result_fp \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mresults/summary.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m summary \u001b[39m=\u001b[39m summarize(input_fp, result_fp, model, max_length\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(summary)\n",
      "\u001b[1;32m/Users/camilleko/NLP_summarization/ext_summarize.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mindex 5\u001b[39m\u001b[39m'\u001b[39m,input_data[\u001b[39m5\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m        \u001b[39m# src, mask, segs, clss, mask_cls, src_str = input_data\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m test(model, input_data, result_fp, max_length, block_trigram\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m \u001b[39mif\u001b[39;00m return_summary:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39m(result_fp)\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39mstrip()\n",
      "\u001b[1;32m/Users/camilleko/NLP_summarization/ext_summarize.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtest-----index 4\u001b[39m\u001b[39m'\u001b[39m,input_data[\u001b[39m4\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtest-----index 5\u001b[39m\u001b[39m'\u001b[39m,input_data[\u001b[39m5\u001b[39m])\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m sent_scores, mask \u001b[39m=\u001b[39m model(src, segs, clss, mask, mask_cls)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtest-----sent_scores\u001b[39m\u001b[39m'\u001b[39m,sent_scores)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtest-----mask\u001b[39m\u001b[39m'\u001b[39m,mask)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/camilleko/NLP_summarization/ext_summarize.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mlen[1] [0] top_vec size \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(top_vec[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m# # Check if top_vec is a tuple and extract the tensor from it\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# if isinstance(top_vec, tuple):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m# # Assuming the tensor is the first element of the tuple\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39m#     top_vec = top_vec[0]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m sents_vec \u001b[39m=\u001b[39m top_vec[torch\u001b[39m.\u001b[39;49marange(top_vec[\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), clss]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msents_vec\u001b[39m\u001b[39m'\u001b[39m, sents_vec)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W2sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msents_vec shape\u001b[39m\u001b[39m'\u001b[39m, sents_vec\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: arange(): argument 'end' (position 1) must be Number, not Tensor"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from SUMM.model_builder import ExtSummarizer\n",
    "# from ext_sum import summarize\n",
    "\n",
    "# Load model\n",
    "checkpoint_pth='./results/bert_ex_s3/Best_stat_dic_exBERTe2_b32_lr1e-05.pth'\n",
    "# checkpoint_pth='/Users/camilleko/NLP_summarization/results/bert_C_ex_s3/model_step_100.pt'\n",
    "# checkpoint_url = './results/bert_ex_s3/Best_stat_dic_exBERTe2_b32_lr1e-05.pth'\n",
    "# checkpoint_url = './results/bertext_cnndm_transformer.pt'\n",
    "device = 'cpu'\n",
    "checkpoint = torch.load(checkpoint_pth, map_location=device)\n",
    "# for k,v in checkpoint['opt']:\n",
    "#     print(k)\n",
    "model = ExtSummarizer(device=device , checkpoint = checkpoint )\n",
    "# model = ExtSummarizer()x\n",
    "\n",
    "\n",
    "# Run summarization\n",
    "input_fp = 'input.txt'\n",
    "result_fp = 'results/summary.txt'\n",
    "summary = summarize(input_fp, result_fp, model, max_length=3)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/camilleko/NLP_summarization/ext_summarize.ipynb Cell 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# You can change this to 'cuda' if you have a GPU\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Load the checkpoint data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(checkpoint_pth, map_location\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Extract the 'model' from the checkpoint\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camilleko/NLP_summarization/ext_summarize.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model \u001b[39m=\u001b[39m checkpoint[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/serialization.py:1165\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m mod_name \u001b[39m=\u001b[39m load_module_mapping\u001b[39m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(mod_name, name)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_transformers import BertModel, BertConfig\n",
    "from SUMM.encoder import ExtTransformerEncoder  # Import your custom modules\n",
    "\n",
    "model_name = \"bert-base-uncased\"  # You can choose a different pre-trained model\n",
    "\n",
    "class CustomBertWithExtractiveSummarization(nn.Module):\n",
    "    def __init__(self, bert_config_path, ext_layer_config):\n",
    "        super(CustomBertWithExtractiveSummarization, self).__init__()\n",
    "        \n",
    "        # Load the BERT model\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Initialize the extractive summarization layer\n",
    "        self.ext_layer = ExtTransformerEncoder(\n",
    "            self.bert.config.hidden_size, d_ff=2048, heads=8, dropout=0.2, num_inter_layers=2\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT embeddings\n",
    "        bert_outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Apply extractive summarization layer\n",
    "        ext_sum_outputs = self.ext_layer(bert_outputs[0], attention_mask)\n",
    "        \n",
    "        return ext_sum_outputs\n",
    "\n",
    "# Load the checkpoint from a file\n",
    "checkpoint_pth = '/Users/camilleko/NLP_summarization/results/bert_C_ex_s3/model_step_100.pt'\n",
    "device = 'cpu'  # You can change this to 'cuda' if you have a GPU\n",
    "\n",
    "# Load the checkpoint data\n",
    "checkpoint = torch.load(checkpoint_pth, map_location=device)\n",
    "\n",
    "# Extract the 'model' from the checkpoint\n",
    "model = CustomBertWithExtractiveSummarization(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "from preprocess import data_builder\n",
    "from models import data_loader\n",
    "from models.data_loader import load_text\n",
    "from models.model_builder import ExtSummarizer\n",
    "from models.trainer import build_trainer\n",
    "from models.data_loader import load_text\n",
    "from models.model_builder import ExtSummarizer\n",
    "from models.trainer import build_trainer\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def load_your_data():\n",
    "    # Replace this with your data loading code.\n",
    "    # Your data should be in the format required by PreSumm.\n",
    "    # You need to create a list of documents for summarization.\n",
    "    # Each document should be a string.\n",
    "\n",
    "    your_documents = [\n",
    "        \"Your first document goes here.\",\n",
    "        \"Your second document goes here.\",\n",
    "        # Add more documents as needed.\n",
    "    ]\n",
    "\n",
    "    return your_documents\n",
    "\n",
    "def extractive_summarization(your_documents):\n",
    "    # Load the pretrained model checkpoint\n",
    "    pretrained_path = \"/path/to/bertext_cnndm_transformer\"  # Replace with your actual path\n",
    "    checkpoint = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    # Initialize the summarizer\n",
    "    model = ExtSummarizer(device=\"cpu\", checkpoint=checkpoint)\n",
    "\n",
    "    # Prepare your data for summarization\n",
    "    documents = [{\"src\": doc, \"labels\": [\"\"]} for doc in your_documents]\n",
    "\n",
    "    # Build data loader\n",
    "    train_iter = data_loader.Dataloader(args, documents, model.vocab, device=\"cpu\", shuffle=False)\n",
    "    trainer = build_trainer(args, device_id, model, None)\n",
    "\n",
    "    # Summarize your documents\n",
    "    for batch in train_iter:\n",
    "        with torch.no_grad():\n",
    "            trainer.test(batch)\n",
    "\n",
    "    # Get the extracted summaries\n",
    "    all_generated_ids, _, _ = trainer.get_output_texts()\n",
    "\n",
    "    # Print the summaries\n",
    "    for i, summary_ids in enumerate(all_generated_ids):\n",
    "        summary = model.decode(summary_ids, model.vocab)\n",
    "        print(f\"Document {i + 1} Summary: {summary}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    your_documents = load_your_data()\n",
    "    extractive_summarization(your_documents)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
